--- 
title: "Week 1"
output:
  html_document: default
  html_notebook: default
--- 

# Basic Business Statistics in R

In my interactions with business professionals who use R, I have frequently heard the importance of reviewing the statistical fundamentals that drive decision-making in the workplace.  The purpose of this notebook is to review the most fundamental elements of statistically sound decision-making using a typical business dataset.  In this notebook, regressions and tests for significance are reviewed.

## Load Packages and Necessary Data

```{r, message=FALSE, warning=FALSE, include=FALSE} 
library(ggvis)
library(caret) 
library(lubridate) 
library(stringr) 
library(broom) 
library(scales) 
library(Hmisc) 
library(magrittr) 
library(tidyverse)

#Load data 
df <- read_csv("data/train.csv") 
stores <- read_csv("data/stores.csv") 
features <- read_csv("data/features.csv")

#Make full data set with all tables combined 
df %<>% 
  left_join(stores, "Store") %>% 
  left_join(features, c("Store", "Date")) %>% 
  select(-IsHoliday.x) %>% 
  rename(IsHoliday = IsHoliday.y)

#Add specific holidays to data set 
df %<>% left_join(data_frame(Date = as.Date(c("2010-02-12", "2011-02-11", "2012-02-10", "2013-02-08", 
                                                "2010-09-10", "2011-09-09", "2012-09-07", "2013-09-06", 
                                                "2010-11-26", "2011-11-25", "2012-11-23", "2013-11-29",
                                                "2010-12-31", "2011-12-30", "2012-12-28", "2013-12-27")),
                               Holiday = c(rep("Superbowl", 4), 
                                           rep("LaborDay", 4), 
                                           rep("Thanksgiving", 4), 
                                           rep("Christmas", 4)
                                           )
                               ), 
                    by = "Date" )

#Look at full dataset 
glimpse(df) 
summary(df)


#Remove extra dataframes from memory 
rm(features, stores) 
```

# Test for a true difference between means

For the first challenge, we will compare stores 20, 4, and 39.  Stores 20 and 4 are ranked #1 and #2 in average weekly sales (out of 45 total stores) while store 39 is ranked #10. Does store 20 perform better than store 4?  Does store 20 perform better than store 39?

To conceptualize the problem, I consider disc golf.  I have certainly played a few rounds where I have performed at par, but does this mean I am par player?  I tend to remember my best rounds and chalk it up to skill while discarding my worst rounds as "bad luck".  The reality is that some of the outcome is skill and some is luck. If I were to play against someone exactly as skilled as myself and win by one stroke, it would not be accurate to say that I am a better player since it was really chance that determined it.  However, how many strokes would I have to win by before you start disbelieving that my competitor really is as good as me: 3? 10? 20?

In the same way, how do we compare the sales performance of several stores? If one store sells 10M dollars in product in one week, but another sells 10,000,001 dollars, would you believe that the second store is better than the first? If we just look at the average weekly sales and say that store 20 outperforms
store 4, while both outperform store 39, the data would appear to support this.  However, this does not take into account the natural variation in each week's sales.  The challenge then is to use the Welch's Studentized T-test to see if there is a true difference between means.


```{r, message=FALSE, warning=FALSE}
#Rank stores to see how weekly averages compare 
df %>% 
  select(Weekly_Sales, Store) %>% 
  group_by(Store) %>% 
  summarise(avg = mean(Weekly_Sales)) %>% 
  arrange(-avg) %>% 
  mutate(rank = rank(-avg)) 
```


## Is there a true difference between the average weekly sales of Store 20 and Store 4?  Compare the distributions and visualize the difference.

```{r, message=FALSE, warning=FALSE}
#Does store 20 perform better than store 4 on average? 
store_20_sales <- df %>% 
  filter(Store == 20) %>% 
  select(Weekly_Sales) %>% 
  unlist 

store_4_sales <- df %>% 
  filter(Store == 4) %>% 
  select(Weekly_Sales) %>% 
  unlist 

twenty_vs_four <- stats::t.test(store_20_sales, store_4_sales, conf.level = .95)

#Distributions of store 20 and store 4 
data_frame(sales = store_20_sales, store = "Store 20") %>% 
  bind_rows(data_frame(sales = store_4_sales, store = "Store4")) %>% 
  ggplot(aes(sales, fill = store)) + 
    geom_density(alpha = .3) + 
    scale_x_log10(limits = c(1, 1000000)) + 
    geom_vline(xintercept = mean(store_20_sales), color = "red") + 
    geom_vline(xintercept = mean(store_4_sales), color = "blue")

#Visualize difference in means w/errorbars for store 20 and store 4 
mean_cl_normal(store_20_sales) %>% 
  bind_rows(mean_cl_normal(store_4_sales)) %>% 
  mutate(store = c("store_20", "store_4")) %>% 
  ggplot(aes(x = store, y = y)) + 
    geom_bar(stat = "identity", fill = "lightgray") + 
    geom_errorbar(aes(ymin = ymin, ymax = ymax), width = .2, color = "red") + 
    labs(title = "No true difference in means between Store 20 and Store 4",
         x = element_blank(), 
         y = "Average Weekly Sales") + 
    scale_x_discrete(labels = c("Store 20", "Store 4")) + 
    scale_y_continuous(labels = scales::dollar) + 
    theme_bw() 
```

## Is there a true difference between the average weekly sales of Store 20 and Store 39?  Compare the distributions and visualize the difference.

```{r, message=FALSE, warning=FALSE}
#Does store 20 perform better than store 39 on average? 
store_39_sales <- df %>% 
  filter(Store == 39) %>% 
  select(Weekly_Sales) %>% 
  unlist 

twenty_vs_thirtynine <- stats::t.test(store_20_sales, store_39_sales, conf.level = .95)

#Distribution of store 20 and store 39 
data_frame(sales = store_20_sales, store = "Store 20") %>% 
  bind_rows(data_frame(sales = store_39_sales, store = "Store 39")) %>% 
  ggplot(aes(sales, fill = store)) + 
    geom_density(alpha = .3) + 
    scale_x_log10(limits = c(1, 1000000)) + 
    geom_vline(xintercept = mean(store_20_sales), color = "red") + 
    geom_vline(xintercept = mean(store_39_sales), color = "blue")

#Visualizing difference in means and errorbars for store 20 and store 39 
mean_cl_normal(store_20_sales) %>% 
  bind_rows(mean_cl_normal(store_39_sales)) %>% 
  mutate(store = c("store_20", "store_39")) %>% 
  ggplot(aes(x = store, y = y)) + 
    geom_bar(stat = "identity", fill = "lightgray") + 
    geom_errorbar(aes(ymin = ymin, ymax = ymax), width = .2, color = "red") + 
    labs(title = "No true difference in means between Store 20 and Store 39", 
         x = element_blank(), 
         y = "Average Weekly Sales") + 
    scale_x_discrete(labels = c("Store 20", "Store 39")) +  
    scale_y_continuous(labels = scales::dollar) + theme_bw() 
```

# Interpreting OLS Linear Regressions

## How can I compare the conditional means of all stores at once?

With summary(lm()), we can run a quick OLS linear regression.  By factoring Store, the model creates a dummy variable for every level of the factor except the first, which is used as the reference case.  Each estimate (or Beta) is the conditional mean of that level when added to the intercept of the model. A p-value is produced for each level, which describes whether the estimate is statistically significant.

```{r, message=FALSE, warning=FALSE}
#Check output of simple linear model 
summary(lm(Weekly_Sales ~ factor(Store), data = df))

#Tidy output, rank, and compare 
lm_simplified_output <- summary(lm(Weekly_Sales ~ factor(Store), data = df)) %>% 
  tidy %>% 
  select(term, estimate, p.value) %>% 
  arrange(abs(estimate)) %>% 
  mutate(p.value = round(p.value, 3), estimate = round(estimate, 1)) %>% 
  slice(c(1:4,45))

#Create a conditional mean for each store in simplifed list 
store1_mean <- df %>% 
  filter(Store == 1) %>% 
  select(Weekly_Sales) %>% 
  unlist %>% 
  mean 

store6_mean <- df %>% 
  filter(Store == 6) %>% 
  select(Weekly_Sales) %>% 
  unlist %>% 
  mean  

store39_mean <- df %>% 
  filter(Store == 39) %>% 
  select(Weekly_Sales) %>% 
  unlist %>% 
  mean 

store19_mean <- df %>% 
  filter(Store == 19) %>% 
  select(Weekly_Sales) %>% 
  unlist %>% 
  mean 

store23_mean <- df %>% 
  filter(Store == 23) %>% 
  select(Weekly_Sales) %>% 
  unlist %>% 
  mean

#Compare mean calculated from data with mean calculated from model output 
lm_simplified_output %>% 
  mutate(means_from_data = c(store6_mean, store39_mean, store19_mean, store23_mean, store1_mean),
         means_from_model = ifelse(term == "(Intercept)", estimate, estimate[5] + estimate)
         ) 
```

## How can I interpret the output of my model?

```{r, message=FALSE, warning=FALSE}
#Available objects in output of model
lm1 <- lm(Weekly_Sales ~ factor(Store), df)
lm1 %>% 
  summary %>% 
  names

#How much of the variation is captured in the model? 
summary(lm1)$r.squared 
summary(lm1)$adj.r.squared

#Plot residuals (errors) 
data_frame(res = summary(lm1)$residuals, index = 1:length(summary(lm1)$residuals)) %>% 
  ggplot(aes(index, res)) + 
    geom_point(alpha = .01, size = .1) + 
    geom_hline(yintercept = 0, color = "red", size = 1, alpha = .4) + 
    geom_smooth(color = "blue", size = .7) + 
    coord_cartesian(ylim = c(-25000, 25000)) + 
    labs(title = "Do the residuals (observed - fitted) look non-systematic? ", 
         y = "Residuals", 
         x = element_blank()) + 
    theme_bw() +  
    theme(axis.text.x = element_blank(), 
          axis.ticks.x = element_blank()) +  
    scale_y_continuous(label = dollar)

#Look at 4 diagnostic plots. Clearly  these are some very strange diagnostics 
par(mfrow = c(2,2)) # Change the panel layout to 2 x 2 
plot(lm1) 

#What would diagnostic plots of a boolean variable look like? 
lm(Weekly_Sales ~ IsHoliday, df) %>% 
  plot

#What would diagnostic plots of a continuous variable look like? 
lm(Weekly_Sales ~ Size, df) %>% 
  plot

#What were those big outliers (Cook's Distance)? Black Friday!! 
df %>% 
  select(Store, Date, IsHoliday, Weekly_Sales) %>% 
  slice(c(95374, 338014))

#How about diagnostic plots with all variables? 
lm(Weekly_Sales ~ ., df) %>% 
  plot 

par(mfrow = c(1,1)) # Most prefer to change immediately back to 1 x 1 to avoid later surprises

#Residuals vs Fitted Plot: Are there non-linear patterns? Why does model underestimate more at higher values? 
##Normal Q-Q Plot: Deeply concerned about the trend on the right, shoudl follow line 
###Scale-Location Plot: Again clear differences between right and left 
####Residuals vs Leverage Plot: Strange clustering (holidays?), but no single point exerts too much leverage.  Could try investigating or removing labelled points

#Is there anything special about the observations with highest leverage? Black Friday!
df %>% 
  select(Store, Date, IsHoliday, Weekly_Sales) %>% 
  slice(c(224464, 37254, 954266)) 
```

#Controlling for other variables

## Is Store the only variable that affects weekly sales?

We might hypothesize that if the week contains a significant holiday, that may influence the weekly sales. The holidays recorded are Superbowl, Labor Day, Thanksgiving, and Christmas.  In the formula for the linear model, the dependent variable is on the left and the independent variable is on the right of the tilde.  To add a control, simply append it with a plus sign as another independent variable.  The first order of business is to factor all necessary variables and shuffle into random order as per diagnostic plots above.


```{r, message=FALSE, warning=FALSE}
#Factor necessary variables and basic feature engineering 
df %<>% mutate(Store = factor(Store), 
                  Dept = factor(Dept), 
                  Type = factor(Type),  
                  IsHoliday = factor(IsHoliday), 
                  Holiday = ifelse(is.na(Holiday), "None", Holiday), 
                  Holiday = factor(Holiday), 
                  MarkDown1 = ifelse(is.na(MarkDown1), 0, MarkDown1), 
                  MarkDown2 = ifelse(is.na(MarkDown2), 0, MarkDown2), 
                  MarkDown3 = ifelse(is.na(MarkDown3), 0, MarkDown3), 
                  MarkDown4 = ifelse(is.na(MarkDown4), 0, MarkDown4), 
                  MarkDown5 = ifelse(is.na(MarkDown5), 0, MarkDown5), 
                  Size = as.numeric(Size), 
                  Month = factor(month(Date)), 
                  Year = year(Date), 
                  Quarter = factor(quarter(Date))
                  )

#Shuffle dataset into random order 
df <- df[sample(1:nrow(df)), ]

####

#Sales on holiday weeks are an average of 1134 more and difference is statistically significant. Lets use IsHoliday as a control 
summary(lm(Weekly_Sales ~ IsHoliday, data = df)) %>% 
  tidy

#Notice that controlling for Holidays has very little effect on the conditional means of each store. In addition, the effect size of whether a week contains one of these four holidays is very small.  However, try replacing IsHoliday with Holiday and note the difference in the output. 
controlled <- summary(lm(Weekly_Sales ~ Store + IsHoliday, data = df)) %>% 
  tidy %>%  
  mutate(IsStore = str_detect(term, "Store"), rank = rank(-abs(estimate)), control = 1) %>% 
  filter(!IsStore | rank(-abs(estimate)) < 5) %>% 
  select(rank, term, estimate, p.value)

uncontrolled <- summary(lm(Weekly_Sales ~ Store, data = df)) %>% 
  tidy %>%
  mutate(IsStore = str_detect(term, "Store"), 
         rank = rank(-abs(estimate)), 
         control = 0) %>% 
  filter(!IsStore | rank(-abs(estimate)) < 5) %>% 
  select(rank, term, estimate, p.value)

controlled %>% 
  bind_rows(uncontrolled)

#What is the difference in average weekly sales between the best performing and worst performing stores?
summary(lm(Weekly_Sales ~ Store, data = df)) %>% 
  tidy %>% 
  mutate(intercept = estimate[1], 
         cond_mean = ifelse(estimate == intercept, estimate, intercept + estimate), 
         rank = rank(cond_mean)) %>% 
  filter(str_detect(term, "Store")) %>%  
  filter(rank == max(rank) | rank == min(rank)) %>% 
  select(rank, cond_mean, term, estimate) %>% 
  summarise(Performance_difference_between_best_and_worst = sum(abs(estimate))) 
#$24,454.89

#What is the difference in average weekly sales between the best performing and worst performing stores when controlling for all avialable features in the model?  What does this mean?
summary(lm(Weekly_Sales ~ ., data = df[,c(1:16,18:20)])) %>% 
  tidy %>%  
  filter(str_detect(term, "Store5") | str_detect(term, "Store20")) %>%
  summarise(Performance_diff_when_controlling_for_all_vars = sum(abs(estimate))) 
#$27,171.95
```

# How can I measure the performance of my model?

All models try to approximate a real world system. Some portion of real-world systems are systematic and therefore, can be modelled.  However, the other portion of the real-world system is random and cannot be modelled.  "All models are wrong, but some are useful".  The best performing models have captured nearly all of the systematic variance of the real-world system without modelling the random portion of the system.  For an in-depth understanding, read about the "Bias-Variance Tradeoff".  In reality, the closer we get to modelling all of the systematic variance, the greater the risk we have of modellign the random variance, a problem known as overfitting.  When we do this, the model does not work as well on new data or we may make incorrect inferences. Lets split the data into a train and test set, optimize the model for the train set and see how it compares on the test set.

```{r, message=FALSE, warning=FALSE}
set.seed(1234)

#Add index column
df %<>% 
  mutate(index = 1:nrow(.))

#Build test set out of 30% of total data
test <- sample_frac(df, 0.3)

#Rebuild train set out of rows not in test set
train <- df %>% 
  filter(!index %in% test$index)

#Build linear model on train set
split_lm <- lm(Weekly_Sales ~ Store + Dept + Date + Type + Size + Temperature + Fuel_Price + CPI + Unemployment + Month + Year, train)

#Average error of prediction, for reference, the Kaggle winner was RMSE = 2301. This score ($13,193) would rank somewhere around 570 out of 690 submissions.
split_lm_test_error <- RMSE(predict(split_lm, test), test$Weekly_Sales)

#What if we split data again?
test <- sample_frac(df, 0.3)
train <- filter(df, !index %in% test$index)
split_lm2 <- lm(Weekly_Sales ~ Store + Dept + Date + Type + Size + Temperature + Fuel_Price + CPI + Unemployment + Month + Year, train)
split_lm2_test_error <- RMSE(predict(split_lm2, test), test$Weekly_Sales)

#Notice that the error changes a little every time we run it
split_lm_test_error
split_lm2_test_error

#How do we get a sense of how the model will perform on some unknown test set?
#One slow and complicated way is to write a loop to do this a bunch of times, then aggregate the results.

fold_and_model <- function(dataframe, iterations) {
  #Model will create a fold of 70/30, run model on 70 and calculate RMSE on 30
  
  #Initialize container for results
  results <- data_frame(rmse = numeric())
  
  for (i in 1:iterations) {
    test <- sample_frac(df, 0.3)
    train <- filter(df, !index %in% test$index)
    lm <- lm(Weekly_Sales ~ Store + Dept + Date + Type + Size + 
                      Temperature + Fuel_Price + CPI + Unemployment + Month + Year, 
                    train)
    error <- RMSE(predict(lm, test), test$Weekly_Sales)
    results[i,] <-  error
  }
  
  print(paste0("The average is ", mean(results$rmse)))
  print(paste0("The std dev is ", sd(results$rmse)))
  ggplot(results, aes(rmse)) + geom_density()
}

fold_and_model(df, 10)

#Thus, we are "pretty sure" this model will produce an average error of 13,200 (give or take 100). How can we quantify what we mean by pretty sure?
```

# Confidence Intervals 

Statistics is sometimes referred to as the "science of uncertainty".  It is an attempt at recognizing, quantifying, and communicating insights along with their associated uncertainty.  To get an intuition for how uncertainty in models can look, we build a dataframe out of 

```{r, message=FALSE, warning=FALSE}
#Change proportion of sample size to see what happens to confidence interval
sample_fraction = .001

#Change level of confidence interval
confidence_level = .95

#Predict Sales from Size of Store
size_lm <- lm(Weekly_Sales ~ Size, train)

#Build dataframe of predictions and confidence levels
predictions <- predict(size_lm, 
                       test, 
                       #interval = "confidence",
                       level = .1,
                       se.fit = T) %>% 
  as_tibble %>% 
  select(fit, se.fit) %>% 
  mutate(upper = fit + qnorm(1 - ((1 - confidence_level) / 2), 0, 1) * se.fit,
         lower = fit - qnorm(1 - ((1 - confidence_level) / 2), 0, 1) * se.fit,
         spread = upper - lower) %>% 
  bind_cols(select(test, Size)) %>% 
  arrange(Size) %>% 
  unique %>% 
  arrange(Size) %>% 
  select(Size, lower, fit, upper, spread, se.fit)

predictions
```


```{r, message=FALSE, warning=FALSE}
#Experiment with size of sample and confidence level
sample_fraction = .001
confidence_level = .95

#Look at confidence interval of prediction
sample_frac(train, sample_fraction) %>% 
  ggplot(aes(Size, Weekly_Sales)) +
    #Plot train dataset
    geom_point(color = "blue", alpha = .3) +
    #Plot confidence interval around line of fit of train data
    geom_smooth(method = "lm", color = "blue", fill = "blue", alpha = .3) +
    #Plot fitted points from test data
    geom_point(data = predictions, aes(Size, fit), color = "red") +
    #Plot error bars on test data set
    stat_summary(geom = "errorbar", 
                 fun.data = mean_cl_normal, 
                 #width = 0.1, 
                 conf.int = confidence_level, 
                 color = "red", 
                 alpha = .3) +
    labs(title = "More observations will shrink the size of the errorbars...",
         subtitle = "...while demanding higher certainty grows the errorbars",
         x = "Size of store (units unknown)",
         y = "Weely Sales")
```

# Interaction Effects

We have reviewed how to model the relationship between one dependent and one indepedent variable.  We have alos modelled the relationship when adding several other independent variables as controls. However, we might still be interested in how the relationship between the dependent and the independent variable might change relative to another variable.

For example, one might hypothesize that poor weather will increase the accident rate in a certain area.  One might go further, though, and say that the effect of weather on accident rates is stronger in the dark than in the daylight.  How would one test for these interaction effects between variables?

In the example below, we will test two hypotheses:
1) That unemployment rates have a different impact on sales during a holiday, and
2) That the extra performance of Dept 92 is different during a holiday.

```{r, message=FALSE, warning=FALSE}
print("What is the effect of unemployment on sales?")
summary(lm(Weekly_Sales ~ Unemployment, train))
print("What if we control for IsHoliday?")
summary(lm(Weekly_Sales ~ Unemployment + IsHoliday, train))
print("Is there an interaction effect? (Is the relationship of Unemployment and Sales different on holidays?)")
summary(lm(Weekly_Sales ~ Unemployment * IsHoliday, train))

print("What is the impact of Dept 92 on Sales?")
train %>% 
  select(Weekly_Sales, Dept) %>% 
  mutate(IsDept92 = Dept == 92) %>% 
  lm(Weekly_Sales ~ IsDept92, .) %>% 
  summary

print("What if you control for IsHoliday?")
train %>% 
  select(Weekly_Sales, Dept, IsHoliday) %>% 
  mutate(IsDept92 = Dept == 92) %>% 
  lm(Weekly_Sales ~ IsDept92 + IsHoliday, .) %>% 
  summary

print("Is there an interaction effect of unemployment rate on performance of Dept 92? Probably a discretionary item.")
train %>% 
  select(Weekly_Sales, Dept, IsHoliday) %>% 
  mutate(IsDept92 = Dept == 92) %>% 
  lm(Weekly_Sales ~ IsDept92 * IsHoliday, .) %>% 
  summary
```











