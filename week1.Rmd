--- 
title: "Week 1"
output:
  html_document: default
  html_notebook: default
--- 

# Basic Business Statistics in R

In my interactions with business professionals who use R, I have frequently heard the importance of reviewing the statistical fundamentals that drive decision-making in the workplace.  The purpose of this notebook is to review the most fundamental elements of statistically sound decision making using a typical business dataset.  In this notebook, regressions and tests for significance are reviewed.

## Load Packages and Necessary Data

To begin, we will need to load all required libraries, load the data, and combine into one dataframe.  This data comes from real sales data for a national chain that has been made public via Kaggle.

```{r, message=FALSE, warning=FALSE}
library(ggvis)
library(caret) 
library(lubridate) 
library(stringr) 
library(broom) 
library(scales) 
library(Hmisc) 
library(magrittr) 
library(tidyverse)

#Load data 
df <- read_csv("data/train.csv") 
stores <- read_csv("data/stores.csv") 
features <- read_csv("data/features.csv")

#Make full data set with all tables combined 
df %<>% 
  left_join(stores, "Store") %>% 
  left_join(features, c("Store", "Date")) %>% 
  select(-IsHoliday.x) %>% 
  rename(IsHoliday = IsHoliday.y)

#Add specific holidays to data set 
df %<>% left_join(data_frame(Date = as.Date(c("2010-02-12", "2011-02-11", "2012-02-10", "2013-02-08", 
                                              "2010-09-10", "2011-09-09", "2012-09-07", "2013-09-06", 
                                              "2010-11-26", "2011-11-25", "2012-11-23", "2013-11-29",
                                              "2010-12-31", "2011-12-30", "2012-12-28", "2013-12-27")),
                               Holiday = c(rep("Superbowl", 4), 
                                           rep("LaborDay", 4), 
                                           rep("Thanksgiving", 4), 
                                           rep("Christmas", 4)
                                           )
                               ), 
                    by = "Date" )

#Look at full dataset 
#glimpse(df) 
summary(df)

#Remove extra dataframes from memory 
rm(features, stores) 

#Write full dataframe back to csv
df %>% write_csv("data/full_data.csv")
```

# Test for a true difference between means

One challenge of business decision making is determining whether an observation represents a true picture of the world.  Since we can never account for every single factor in a model, all models are imperfect and wrong (though many are still useful).  Examples of this problem arise when managers must decide whether a there is a true difference in performance between sales staff, departments, or regions.  How much of a difference is enough to conclude that one store is performing better than another?

To conceptualize the problem, I consider playing disc golf with my wife.  If I watch one player beat another by one stroke over 18 holes, I would not feel confident to conclude that she is better than her peer (it woudl seem plausible that the difference could just be luck).  However, is she beat her peer by 15 strokes in one game, I would feel pretty confident that she is better.  Alternatively, if she beat her peer by one stroke every single time over 150 games, I would feel pretty confident that she is marginally better.  Our confidence that there is a difference in performance between two players can increase with either a bigger difference in performance or a greater number of observations.

In the same way, we can compare the sales performance of several stores in this dataset. If one store sells 10M dollars in product in one week, but another sells 10,000,001 dollars, would you believe that the second store is better than the first? In this data set we can look at the average weekly sales of each store and see that Store #20 outperforms Store #4, while both outperform Store #39. However, this does not take into account the natural variation in each week's sales.  The challenge then is to use the Welch's Studentized T-test to see if there is a true difference between means.

For the first challenge, we will compare stores #20, #4, and #39.  Stores #20 and #4 are ranked #1 and #2 in average weekly sales (out of 45 total stores) while store #39 is ranked #10. Does store #20 perform better than store #4?  Does store #20 perform better than store #39?

```{r, message=FALSE, warning=FALSE}
#Rank stores to see how weekly averages compare 
df %>% 
  select(Weekly_Sales, Store) %>% 
  group_by(Store) %>% 
  summarise(avg = mean(Weekly_Sales)) %>% 
  arrange(-avg) %>% 
  mutate(rank = rank(-avg))
```


## Is there a true difference between the average weekly sales of Store 20 and Store 4?  Compare the distributions and visualize the difference.

```{r, message=FALSE, warning=FALSE}
#Does store 20 perform better than store 4 on average? 
store_20_sales <- df %>% 
  filter(Store == 20) %>% 
  select(Weekly_Sales) %>% 
  unlist 

store_4_sales <- df %>% 
  filter(Store == 4) %>% 
  select(Weekly_Sales) %>% 
  unlist 

twenty_vs_four <- stats::t.test(store_20_sales, store_4_sales, conf.level = .95)
twenty_vs_four

#Distributions of store 20 and store 4 
data_frame(sales = store_20_sales, store = "Store 20") %>% 
  bind_rows(data_frame(sales = store_4_sales, store = "Store4")) %>% 
  ggplot(aes(sales, fill = store)) + 
    geom_density(alpha = .3) + 
    scale_x_log10(limits = c(1, 1000000), labels = dollar) + 
    geom_vline(xintercept = mean(store_20_sales), color = "red") + 
    geom_vline(xintercept = mean(store_4_sales), color = "blue") +
    labs(title = "Very little difference in the distributions between Store #20 and Store #4",
         x = "Sales",
         y = "Distribution")

#Visualize difference in means w/errorbars for store 20 and store 4 
mean_cl_normal(store_20_sales) %>% 
  bind_rows(mean_cl_normal(store_4_sales)) %>% 
  mutate(store = c("store_20", "store_4")) %>% 
  ggplot(aes(x = store, y = y)) + 
    geom_bar(stat = "identity", fill = "lightgray", width = .5) + 
    geom_errorbar(aes(ymin = ymin, ymax = ymax), width = .2, color = "red") + 
    labs(title = "No true difference in means between Store 20 and Store 4",
         x = element_blank(), 
         y = "Average Weekly Sales") + 
    scale_x_discrete(labels = c("Store 20", "Store 4")) + 
    scale_y_continuous(labels = scales::dollar) + 
    theme_bw() 
```

## Is there a true difference between the average weekly sales of Store 20 and Store 39?  Compare the distributions and visualize the difference.

```{r, message=FALSE, warning=FALSE}
#Does store 20 perform better than store 39 on average? 
store_39_sales <- df %>% 
  filter(Store == 39) %>% 
  select(Weekly_Sales) %>% 
  unlist 

twenty_vs_thirtynine <- stats::t.test(store_20_sales, store_39_sales, conf.level = .95)
twenty_vs_thirtynine

#Distribution of store 20 and store 39 
data_frame(sales = store_20_sales, store = "Store 20") %>% 
  bind_rows(data_frame(sales = store_39_sales, store = "Store 39")) %>% 
  ggplot(aes(sales, fill = store)) + 
    geom_density(alpha = .3) + 
    scale_x_log10(limits = c(1, 1000000)) + 
    geom_vline(xintercept = mean(store_20_sales), color = "red") + 
    geom_vline(xintercept = mean(store_39_sales), color = "blue") +
    labs(title = "Noticeable difference in the distributions between Store #20 and Store #39",
         x = "Sales",
         y = "Distribution")

#Visualizing difference in means and errorbars for store 20 and store 39 
mean_cl_normal(store_20_sales) %>% 
  bind_rows(mean_cl_normal(store_39_sales)) %>% 
  mutate(store = c("store_20", "store_39")) %>% 
  ggplot(aes(x = store, y = y)) + 
    geom_bar(stat = "identity", fill = "lightgray") + 
    geom_errorbar(aes(ymin = ymin, ymax = ymax), width = .2, color = "red") + 
    labs(title = "No true difference in means between Store 20 and Store 39", 
         x = element_blank(), 
         y = "Average Weekly Sales") + 
    scale_x_discrete(labels = c("Store 20", "Store 39")) +  
    scale_y_continuous(labels = scales::dollar) + 
    theme_bw() 

```

# Interpreting OLS Linear Regressions

## How can I compare the conditional means of all stores at once?

There are 45 distinct stores in this dataset.  It would be painstaking to test for a true difference in means for each combination.  Instead, we can run a quick OLS linear regression in a single line of code with summary(lm()).  By factoring Store, the model creates a dummy variable for every level of the factor except the first, which is used as the reference case.  Each estimate (or Beta) is the conditional mean of that level when added to the intercept of the model. A p-value is produced for each level, which describes whether the estimate is statistically significant.  Here, we compare the output of the linear model with the independent conditional means of several test cases.

```{r, message=FALSE, warning=FALSE}
#Check output of simple linear model 
summary(lm(Weekly_Sales ~ factor(Store), data = df))

#Tidy output, rank, and compare 
lm_simplified_output <- summary(lm(Weekly_Sales ~ factor(Store), data = df)) %>% 
  tidy %>% 
  select(term, estimate, p.value) %>% 
  arrange(abs(estimate)) %>% 
  mutate(p.value = round(p.value, 3), estimate = round(estimate, 1)) %>% 
  slice(c(1:4,45))

#Create a conditional mean for each store in simplifed list 
store1_mean <- df %>% 
  filter(Store == 1) %>% 
  select(Weekly_Sales) %>% 
  unlist %>% 
  mean 

store6_mean <- df %>% 
  filter(Store == 6) %>% 
  select(Weekly_Sales) %>% 
  unlist %>% 
  mean  

store39_mean <- df %>% 
  filter(Store == 39) %>% 
  select(Weekly_Sales) %>% 
  unlist %>% 
  mean 

store19_mean <- df %>% 
  filter(Store == 19) %>% 
  select(Weekly_Sales) %>% 
  unlist %>% 
  mean 

store23_mean <- df %>% 
  filter(Store == 23) %>% 
  select(Weekly_Sales) %>% 
  unlist %>% 
  mean

#Compare mean calculated from data with mean calculated from model output 
lm_simplified_output %>% 
  mutate(means_from_data = c(store6_mean, store39_mean, store19_mean, store23_mean, store1_mean),
         means_from_model = ifelse(term == "(Intercept)", estimate, estimate[5] + estimate)
         ) 
```

## How can I interpret the output of my model?

The output for a simple linear regression is simple to interpret and useful for prediction and inference.  Here, we review the available objects, some basic performance measures, and introductory diagnostics.

```{r, message=FALSE, warning=FALSE}
#Available objects in output of model
lm1 <- lm(Weekly_Sales ~ factor(Store), df)
lm1 %>% 
  summary %>% 
  names

#How much of the variation is captured in the model? 
summary(lm1)$r.squared 
summary(lm1)$adj.r.squared

#Plot residuals (errors) 
data_frame(res = summary(lm1)$residuals, index = 1:length(summary(lm1)$residuals)) %>% 
  ggplot(aes(index, res)) + 
    geom_point(alpha = .01, size = .1) + 
    geom_hline(yintercept = 0, color = "red", size = 1, alpha = .4) + 
    geom_smooth(color = "blue", size = .7, se = T) + 
    coord_cartesian(ylim = c(-25000, 25000)) + 
    labs(title = "Do the residuals (observed - fitted) look non-systematic? ", 
         y = "Residuals", 
         x = element_blank()) + 
    theme_bw() +  
    theme(axis.text.x = element_blank(), 
          axis.ticks.x = element_blank()) +  
    scale_y_continuous(label = dollar)

#Look at 4 diagnostic plots. Clearly  these are some very strange diagnostics 
par(mfrow = c(2,2)) # Change the panel layout to 2 x 2 
plot(lm1) 

#What would diagnostic plots of a boolean variable look like? 
lm(Weekly_Sales ~ IsHoliday, df) %>% 
  plot

#What would diagnostic plots of a continuous variable look like? 
lm(Weekly_Sales ~ Unemployment, df) %>% 
  plot

#What were those big outliers (Cook's Distance)? Black Friday!! 
df %>% 
  select(Store, Date, IsHoliday, Weekly_Sales) %>% 
  slice(c(95374, 338014))

#How about diagnostic plots with all variables? 
lm(Weekly_Sales ~ ., df) %>% 
  plot 

par(mfrow = c(1,1)) # Most prefer to change immediately back to 1 x 1 to avoid later surprises

#Residuals vs Fitted Plot: Are there non-linear patterns? Why does model underestimate more at higher values? 
##Normal Q-Q Plot: Deeply concerned about the trend on the right, shoudl follow line 
###Scale-Location Plot: Again clear differences between right and left 
####Residuals vs Leverage Plot: Strange clustering (holidays?), but no single point exerts too much leverage.  Could try investigating or removing labelled points

#Is there anything special about the observations with highest leverage? Also Black Friday!
df %>% 
  select(Store, Date, IsHoliday, Weekly_Sales) %>% 
  slice(c(224464, 37254, 954266)) 
```

#Controlling for other variables

The shortcoming of univariate regression is that it cannot account for other possible explanations.  For example one might discover that there is a strong relationship between height likelihood of being incarcerated.  However, when controlling for gender, we find that males are incarcerated at significantly higher rates than females and also tend to be taller.  When the "control" (adding "gender" as another independent variable) is added to the model, the relationship between height and incarceration disappears, and it is found that gender is the explanatory variable.

## Is Store the only variable that affects weekly sales?

Similarly, we might hypothesize that if the week contains a significant holiday, that may influence the weekly sales. The holidays recorded are Superbowl, Labor Day, Thanksgiving, and Christmas.  In the formula for the linear model, the dependent variable is on the left and the independent variable is on the right of the tilde.  To add a control, simply append it with a plus sign as another independent variable.  The first order of business is to factor all necessary variables and shuffle into random order.  Next, interpret the results of the linear models with controls.


```{r, message=FALSE, warning=FALSE}
#Factor necessary variables and basic feature engineering 
df %<>% mutate(Store = factor(Store), 
                  Dept = factor(Dept), 
                  Type = factor(Type),  
                  IsHoliday = factor(IsHoliday), 
                  Holiday = ifelse(is.na(Holiday), "None", Holiday), 
                  Holiday = factor(Holiday), 
                  MarkDown1 = ifelse(is.na(MarkDown1), 0, MarkDown1), 
                  MarkDown2 = ifelse(is.na(MarkDown2), 0, MarkDown2), 
                  MarkDown3 = ifelse(is.na(MarkDown3), 0, MarkDown3), 
                  MarkDown4 = ifelse(is.na(MarkDown4), 0, MarkDown4), 
                  MarkDown5 = ifelse(is.na(MarkDown5), 0, MarkDown5), 
                  Size = as.numeric(Size), 
                  Month = factor(month(Date)), 
                  Year = year(Date), 
                  Quarter = factor(quarter(Date))
                  )

#Shuffle dataset into random order 
df <- df[sample(1:nrow(df)), ]

####

#Sales on holiday weeks are an average of 1134 more and difference is statistically significant. Lets use IsHoliday as a control 
summary(lm(Weekly_Sales ~ IsHoliday, data = df)) %>% 
  tidy

#Notice that controlling for Holidays has very little effect on the conditional means of each store. In addition, the effect size of whether a week contains one of these four holidays is very small.  However, try replacing IsHoliday with Holiday and note the difference in the output. 
controlled <- summary(lm(Weekly_Sales ~ Store + IsHoliday, data = df)) %>% 
  tidy %>%  
  mutate(IsStore = str_detect(term, "Store"), rank = rank(-abs(estimate)), control = 1) %>% 
  filter(!IsStore | rank(-abs(estimate)) < 5) %>% 
  select(rank, term, estimate, p.value)

uncontrolled <- summary(lm(Weekly_Sales ~ Store, data = df)) %>% 
  tidy %>%
  mutate(IsStore = str_detect(term, "Store"), 
         rank = rank(-abs(estimate)), 
         control = 0) %>% 
  filter(!IsStore | rank(-abs(estimate)) < 5) %>% 
  select(rank, term, estimate, p.value)

controlled %>% 
  bind_rows(uncontrolled)

#What is the difference in average weekly sales between the best performing and worst performing stores?
summary(lm(Weekly_Sales ~ Store, data = df)) %>% 
  tidy %>% 
  mutate(intercept = estimate[1], 
         cond_mean = ifelse(estimate == intercept, estimate, intercept + estimate), 
         rank = rank(cond_mean)) %>% 
  filter(str_detect(term, "Store")) %>%  
  filter(rank == max(rank) | rank == min(rank)) %>% 
  select(rank, cond_mean, term, estimate) %>% 
  summarise(Performance_difference_between_best_and_worst = sum(abs(estimate))) 
#$24,454.89

#What is the difference in average weekly sales between the best performing and worst performing stores when controlling for all avialable features in the model?  What does this mean?
summary(lm(Weekly_Sales ~ ., data = df[,c(1:16,18:20)])) %>% 
  tidy %>%  
  filter(str_detect(term, "Store5") | str_detect(term, "Store20")) %>%
  summarise(Performance_diff_when_controlling_for_all_vars = sum(abs(estimate))) 
#$27,171.95
```

# How can I measure the performance of my model?

All models try to approximate a real world system. Some portion of real-world systems are systematic and therefore, can be modeled.  However, the other portion of the real-world system is random and cannot be modeled.  "All models are wrong, but some are useful".  The best-performing models have captured nearly all of the systematic variance of the real-world system without modeling the random portion of the system.  For an in-depth understanding, read about the "Bias-Variance Tradeoff".  In reality, the closer we get to modeling all of the systematic variance, the greater the risk we have of modeling the random variance, a problem known as overfitting.  When we do this, the model does not work as well on new data or we may make incorrect inferences. Lets split the data into a train and test set, optimize the model for the train set, and see how it compares on the test set.

```{r, message=FALSE, warning=FALSE}
set.seed(1234)

#Add index column
df %<>% 
  mutate(index = 1:nrow(.))

#Build test set out of 30% of total data
test <- sample_frac(df, 0.3)

#Rebuild train set out of rows not in test set
train <- df %>% 
  filter(!index %in% test$index)

#Build linear model on train set
split_lm <- lm(Weekly_Sales ~ Store + Dept + Date + Type + Size + Temperature + Fuel_Price + CPI + Unemployment + Month + Year, train)

#Average error of prediction, for reference, the Kaggle winner was RMSE = 2301. This score ($13,193) would rank somewhere around 570 out of 690 submissions.
split_lm_test_error <- RMSE(predict(split_lm, test), test$Weekly_Sales)

#What if we split data again?
test <- sample_frac(df, 0.3)
train <- filter(df, !index %in% test$index)
split_lm2 <- lm(Weekly_Sales ~ Store + Dept + Date + Type + Size + Temperature + Fuel_Price + CPI + Unemployment + Month + Year, train)
split_lm2_test_error <- RMSE(predict(split_lm2, test), test$Weekly_Sales)

#Notice that the error changes a little every time we run it
split_lm_test_error
split_lm2_test_error

#How do we get a sense of how the model will perform on some unknown test set?
#One slow and complicated way is to write a loop to do this a bunch of times, then aggregate the results.

fold_and_model <- function(dataframe, iterations) {
  #Model will create a fold of 70/30, run model on 70 and calculate RMSE on 30
  
  #Initialize container for results
  results <- data_frame(rmse = numeric())
  
  for (i in 1:iterations) {
    test <- sample_frac(df, 0.3)
    train <- filter(df, !index %in% test$index)
    lm <- lm(Weekly_Sales ~ Store + Dept + Date + Type + Size + 
                      Temperature + Fuel_Price + CPI + Unemployment + Month + Year, 
                    train)
    error <- RMSE(predict(lm, test), test$Weekly_Sales)
    results[i,] <-  error
  }
  
  print(paste0("The average is ", mean(results$rmse)))
  print(paste0("The std dev is ", sd(results$rmse)))
  ggplot(results, aes(rmse)) + geom_density()
}

fold_and_model(df, 10)

#Thus, we are "pretty sure" this model will produce an average error of 13,200 (give or take 100). How can we quantify what we mean by pretty sure?
```

# Confidence Intervals 

Remember, statistics is the "science of uncertainty".  It is an attempt at recognizing, quantifying, and communicating insights along with their associated uncertainty.  To get an intuition for how uncertainty in models can look, we build a dataframe out of all the test set predictions using the model we built on the train dataset.

```{r, message=FALSE, warning=FALSE}
#Change proportion of sample size to see what happens to confidence interval
sample_fraction = .001

#Change level of confidence interval
confidence_level = .95

#Predict Sales from Size of Store
size_lm <- lm(Weekly_Sales ~ Size, train)

#Build dataframe of predictions and confidence levels
predictions <- predict(size_lm, 
                       test, 
                       #interval = "confidence",
                       level = .1,
                       se.fit = T) %>% 
  as_tibble %>% 
  select(fit, se.fit) %>% 
  mutate(upper = fit + qnorm(1 - ((1 - confidence_level) / 2), 0, 1) * se.fit,
         lower = fit - qnorm(1 - ((1 - confidence_level) / 2), 0, 1) * se.fit,
         spread = upper - lower) %>% 
  bind_cols(select(test, Size)) %>% 
  arrange(Size) %>% 
  unique %>% 
  arrange(Size) %>% 
  select(Size, lower, fit, upper, spread, se.fit)

predictions
```

Notice that each prediction ("fit") has a "spread" of uncertainty around it.  How can we understand what this "spread" or uncertainty means?  In the next visualization, the blue points represent the observations in the train set.  A blue linear fit line is plotted along with the blue confidence interval; this represents the best guess of where the points will land in the test set.  On top of that, the actual predictions from the test set are plotted in red, along with the errorbars based on their independent confidence intervals.

If you are viewing this notebook in a running R session, try different values for the confidence level and the fraction of the dataset to sample.  If the confidence level is changed from the standard 95% to another value, what happens to the width of the blue confidence ribbon?  Similarly, if the amount of data used to build the model is changed from 0.01% to a higher fraction fo the data, what happens to the blue confidence ribbon?  What happens to the red errorbars?

```{r, message=FALSE, warning=FALSE}
#Experiment with size of sample and confidence level
sample_fraction = .001
confidence_level = .1

#Look at confidence interval of prediction
sample_frac(train, sample_fraction) %>% 
  ggplot(aes(Size, Weekly_Sales)) +
    #Plot train dataset
    geom_point(color = "blue", alpha = .3) +
    #Plot confidence interval around line of fit of train data
    geom_smooth(method = "lm", color = "blue", fill = "blue", alpha = .3) +
    #Plot fitted points from test data
    geom_point(data = predictions, aes(Size, fit), color = "red") +
    #Plot error bars on test data set
    stat_summary(geom = "errorbar", 
                 fun.data = mean_cl_normal, 
                 #width = 0.1, 
                 conf.int = confidence_level, 
                 color = "red", 
                 alpha = .3) +
    labs(title = "More observations will shrink the size of the errorbars...",
         subtitle = "...while demanding higher certainty grows the errorbars",
         x = "Size of store (units unknown)",
         y = "Weely Sales")
```

Let's also look at the actual variation of the observations around the predictions:

```{r}
test %>% 
  bind_cols(as_tibble(predict(size_lm, ., interval = "confidence"))) %>%
  ggplot(aes(Size, Weekly_Sales)) +
    #Plot test observations
    geom_point(alpha = .01) +
    #Plot test predictions
    geom_point(aes(Size, fit), alpha = .01, color = "red") +
    labs(title = "Notice the variation of the actual observations around the prediction",
         x = "Size of store (units unknown)",
         y = "Weely Sales")


```

# Interaction Effects

We have reviewed how to model the relationship between one dependent and one indepedent variable.  We have also modeled the relationship when adding several other independent variables as controls. However, we might still be interested in how the relationship between the dependent and the independent variable might change relative to another variable.

For example, one might hypothesize that poor weather will increase the accident rate in a certain area.  One might go further, though, and say that the effect of weather on accident rates is stronger in the dark than in the daylight.  How would one test for these interaction effects between variables?

In the example below, we will test two hypotheses:
1) That unemployment rates have a different impact on sales during a holiday, and
2) That the extra performance of Dept 92 is different during a holiday.

```{r, message=FALSE, warning=FALSE}
print("What is the effect of unemployment on sales?")
summary(lm(Weekly_Sales ~ Unemployment, train))
print("What if we control for IsHoliday?")
summary(lm(Weekly_Sales ~ Unemployment + IsHoliday, train))
print("Is there an interaction effect? (Is the relationship of Unemployment and Sales different on holidays?)")
summary(lm(Weekly_Sales ~ Unemployment * IsHoliday, train))

print("What is the impact of Dept 92 on Sales?")
train %>% 
  select(Weekly_Sales, Dept) %>% 
  mutate(IsDept92 = Dept == 92) %>% 
  lm(Weekly_Sales ~ IsDept92, .) %>% 
  summary

print("What if you control for IsHoliday?")
train %>% 
  select(Weekly_Sales, Dept, IsHoliday) %>% 
  mutate(IsDept92 = Dept == 92) %>% 
  lm(Weekly_Sales ~ IsDept92 + IsHoliday, .) %>% 
  summary

print("Is there an interaction effect of unemployment rate on performance of Dept 92? Probably a discretionary item.")
train %>% 
  select(Weekly_Sales, Dept, IsHoliday) %>% 
  mutate(IsDept92 = Dept == 92) %>% 
  lm(Weekly_Sales ~ IsDept92 * IsHoliday, .) %>% 
  summary
```


Thanks to all for sticking with me to the very end.  Happy sciencing!

-Dan